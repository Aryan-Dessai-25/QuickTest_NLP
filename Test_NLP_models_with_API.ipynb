{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Aryan-Dessai-25/QuickTest_NLP/blob/main/Test_NLP_models_with_API.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "44wLxdc_CxEV"
      },
      "outputs": [],
      "source": [
        "# Dependencies Installation\n",
        "# Force reinstall GPU-compatible PyTorch with Triton support\n",
        "!pip uninstall -y torch torchvision torchaudio numpy\n",
        "\n",
        "# Install GPU-compatible PyTorch\n",
        "!pip install --no-cache-dir --upgrade torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
        "\n",
        "# Install all required and compatible packages\n",
        "!pip install --no-cache-dir \\\n",
        "  transformers==4.35.2 \\\n",
        "  sentence-transformers==2.3.1 \\\n",
        "  faiss-cpu==1.7.4 \\\n",
        "  fastapi==0.105.0 \\\n",
        "  uvicorn==0.24.0.post1 \\\n",
        "  python-multipart==0.0.6 \\\n",
        "  pyngrok==7.0.0 \\\n",
        "  langchain==0.0.350 \\\n",
        "  langchain-community==0.0.13 \\\n",
        "  pillow==10.0.1 \\\n",
        "  numpy==1.26.4 \\\n",
        "  psutil \\\n",
        "  peft==0.7.1 \\\n",
        "  nest-asyncio\n",
        "\n",
        "# Restart the runtime after running this cell to apply changes\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from pyngrok import ngrok\n",
        "\n",
        "\n",
        "NGROK_AUTH_TOKEN = \"\"  # Replace this with your own token\n",
        "ngrok.set_auth_token(NGROK_AUTH_TOKEN)"
      ],
      "metadata": {
        "id": "_6O5bZBxEQpg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "MODEL_CONFIG = {\n",
        "    # Summarization models\n",
        "    \"summarization_model\": \"philschmid/bart-large-cnn-samsum\",\n",
        "    \"summarization_peft_adapter\": None,  # Set to adapter path if needed\n",
        "\n",
        "    # RAG models for extractive QnA\n",
        "    \"embedding_model\": \"sentence-transformers/all-MiniLM-L6-v2\",\n",
        "    \"qa_model\": \"distilbert-base-cased-distilled-squad\",\n",
        "    \"qa_peft_adapter\": None,  # Set to adapter path if needed\n",
        "\n",
        "    # RAG models for abstractive QnA\n",
        "    \"abstractive_embedding_model\": \"sentence-transformers/all-MiniLM-L6-v2\",\n",
        "    \"abstractive_qa_model\": \"microsoft/DialoGPT-small\",  # Lightweight CausalLM\n",
        "    \"abstractive_qa_peft_adapter\": None,  # Set to adapter path if needed\n",
        "\n",
        "    # Alternative models for abstractive QA\n",
        "    # \"abstractive_qa_model\": \"distilgpt2\",  # Even smaller option\n",
        "    # \"abstractive_qa_model\": \"microsoft/DialoGPT-medium\",  # Larger but still T4-friendly\n",
        "\n",
        "    # Sentiment Classification models\n",
        "    \"sentiment_model\": \"cardiffnlp/twitter-roberta-base-sentiment-latest\",\n",
        "    \"sentiment_peft_adapter\": None,  # Set to adapter path if needed\n",
        "\n",
        "    # Named Entity Recognition models\n",
        "    \"ner_model\": \"dbmdz/bert-large-cased-finetuned-conll03-english\",\n",
        "    \"ner_peft_adapter\": None,  # Set to adapter path if needed\n",
        "}\n",
        "\n",
        "print(\"Updated Model Configuration:\")\n",
        "for key, value in MODEL_CONFIG.items():\n",
        "    print(f\"  {key}: {value}\")"
      ],
      "metadata": {
        "id": "Qy853pSwC1wK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extractive Q&A + RAG System with PEFT Support\n",
        "from langchain_community.vectorstores.faiss import FAISS\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.schema import Document as LangchainDocument\n",
        "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
        "from peft import PeftModel\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import gc\n",
        "import importlib\n",
        "\n",
        "class RAGSystem:\n",
        "    def __init__(self):\n",
        "        self.embedding_model = None\n",
        "        self.tokenizer = None\n",
        "        self.qa_model = None\n",
        "        self.vector_store = None\n",
        "\n",
        "        self.embedding_model_name = MODEL_CONFIG[\"embedding_model\"]\n",
        "        self.qa_model_name = MODEL_CONFIG[\"qa_model\"]\n",
        "        self.qa_peft_adapter = MODEL_CONFIG[\"qa_peft_adapter\"]\n",
        "\n",
        "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
        "            chunk_size=500,\n",
        "            chunk_overlap=50,\n",
        "            length_function=len\n",
        "        )\n",
        "\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        print(\"RAG system initialized - models will be loaded when needed\")\n",
        "\n",
        "    def _load_embedding_model(self):\n",
        "        if self.embedding_model is None:\n",
        "            print(\"Loading embedding model...\")\n",
        "            try:\n",
        "                importlib.invalidate_caches()\n",
        "                self.embedding_model = HuggingFaceEmbeddings(\n",
        "                    model_name=self.embedding_model_name,\n",
        "                    model_kwargs={\"device\": self.device}\n",
        "                )\n",
        "            except ImportError:\n",
        "                raise RuntimeError(\n",
        "                    \"sentence-transformers is not installed. \"\n",
        "                    \"Install it using: pip install sentence-transformers\"\n",
        "                )\n",
        "            print(\"Embedding model loaded\")\n",
        "\n",
        "    def _load_qa_model(self):\n",
        "        if self.qa_model is None or self.tokenizer is None:\n",
        "            print(\"Loading QA model...\")\n",
        "            self.tokenizer = AutoTokenizer.from_pretrained(self.qa_model_name)\n",
        "            self.qa_model = AutoModelForQuestionAnswering.from_pretrained(self.qa_model_name)\n",
        "\n",
        "            # Load PEFT adapter if specified\n",
        "            if self.qa_peft_adapter:\n",
        "                print(f\"Loading PEFT adapter: {self.qa_peft_adapter}\")\n",
        "                self.qa_model = PeftModel.from_pretrained(self.qa_model, self.qa_peft_adapter)\n",
        "\n",
        "            self.qa_model.to(self.device)\n",
        "            print(\"QA model loaded\")\n",
        "\n",
        "    def _unload_qa_model(self):\n",
        "        self.qa_model = None\n",
        "        self.tokenizer = None\n",
        "        gc.collect()\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "    def add_documents(self, documents):\n",
        "        try:\n",
        "            self._load_embedding_model()\n",
        "\n",
        "            # Flatten and chunk text\n",
        "            all_chunks = []\n",
        "            for doc in documents:\n",
        "                chunks = self.text_splitter.split_text(doc[\"content\"])\n",
        "                for chunk in chunks:\n",
        "                    all_chunks.append(\n",
        "                        LangchainDocument(page_content=chunk, metadata=doc[\"metadata\"])\n",
        "                    )\n",
        "\n",
        "            if not all_chunks:\n",
        "                return {\"status\": \"error\", \"message\": \"No valid document content to index.\"}\n",
        "\n",
        "            if self.vector_store is None:\n",
        "                self.vector_store = FAISS.from_documents(all_chunks, self.embedding_model)\n",
        "            else:\n",
        "                self.vector_store.add_documents(all_chunks)\n",
        "\n",
        "            return {\n",
        "                \"status\": \"success\",\n",
        "                \"message\": f\"Added {len(documents)} documents with {len(all_chunks)} total chunks.\"\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            return {\n",
        "                \"status\": \"error\",\n",
        "                \"message\": f\"Failed to add documents: {str(e)}\"\n",
        "            }\n",
        "\n",
        "    def answer_question(self, question, top_k=3):\n",
        "        try:\n",
        "            if self.vector_store is None:\n",
        "                return {\"status\": \"error\", \"message\": \"Knowledge base is empty.\"}\n",
        "\n",
        "            self._load_embedding_model()\n",
        "\n",
        "            docs = self.vector_store.similarity_search(question, k=top_k)\n",
        "            contexts = [doc.page_content for doc in docs]\n",
        "            combined_context = \" \".join(contexts)\n",
        "\n",
        "            self._load_qa_model()\n",
        "\n",
        "            inputs = self.tokenizer(\n",
        "                question,\n",
        "                combined_context,\n",
        "                return_tensors=\"pt\",\n",
        "                max_length=512,\n",
        "                truncation=True,\n",
        "                padding=True\n",
        "            )\n",
        "            inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
        "\n",
        "            with torch.no_grad():\n",
        "                outputs = self.qa_model(**inputs)\n",
        "\n",
        "            answer_start = torch.argmax(outputs.start_logits)\n",
        "            answer_end = torch.argmax(outputs.end_logits) + 1\n",
        "\n",
        "            input_ids = inputs[\"input_ids\"][0]\n",
        "            answer = self.tokenizer.convert_tokens_to_string(\n",
        "                self.tokenizer.convert_ids_to_tokens(input_ids[answer_start:answer_end])\n",
        "            )\n",
        "\n",
        "            if not answer.strip():\n",
        "                answer = \"I don't have enough information to answer that question.\"\n",
        "\n",
        "            result = {\n",
        "                \"status\": \"success\",\n",
        "                \"answer\": answer,\n",
        "                \"sources\": [{\"content\": doc.page_content, \"metadata\": doc.metadata} for doc in docs]\n",
        "            }\n",
        "\n",
        "            return result\n",
        "\n",
        "        except Exception as e:\n",
        "            return {\"status\": \"error\", \"message\": str(e)}\n",
        "\n",
        "        finally:\n",
        "            self._unload_qa_model()"
      ],
      "metadata": {
        "id": "U6dmSmMUC8aE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Abstractive Answering RAG System with PEFT Support\n",
        "from langchain_community.vectorstores.faiss import FAISS\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.schema import Document as LangchainDocument\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig\n",
        "from peft import PeftModel\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import gc\n",
        "import importlib\n",
        "import re\n",
        "\n",
        "class AbstractiveRAGSystem:\n",
        "    def __init__(self):\n",
        "        self.embedding_model = None\n",
        "        self.tokenizer = None\n",
        "        self.qa_model = None\n",
        "        self.vector_store = None\n",
        "\n",
        "        self.embedding_model_name = MODEL_CONFIG[\"abstractive_embedding_model\"]\n",
        "        self.qa_model_name = MODEL_CONFIG[\"abstractive_qa_model\"]\n",
        "        self.qa_peft_adapter = MODEL_CONFIG[\"abstractive_qa_peft_adapter\"]\n",
        "\n",
        "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
        "            chunk_size=400,  # Slightly smaller for better context management\n",
        "            chunk_overlap=40,\n",
        "            length_function=len\n",
        "        )\n",
        "\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "        # Generation configuration\n",
        "        self.generation_config = GenerationConfig(\n",
        "            max_new_tokens=150,\n",
        "            min_length=10,\n",
        "            do_sample=True,\n",
        "            temperature=0.7,\n",
        "            top_p=0.9,\n",
        "            top_k=50,\n",
        "            repetition_penalty=1.1,\n",
        "            pad_token_id=None,  # Will be set after loading tokenizer\n",
        "            eos_token_id=None,  # Will be set after loading tokenizer\n",
        "        )\n",
        "\n",
        "        print(\"Abstractive RAG system initialized - models will be loaded when needed\")\n",
        "\n",
        "    def _load_embedding_model(self):\n",
        "        \"\"\"Load the embedding model for document retrieval\"\"\"\n",
        "        if self.embedding_model is None:\n",
        "            print(\"Loading embedding model...\")\n",
        "            try:\n",
        "                importlib.invalidate_caches()\n",
        "                self.embedding_model = HuggingFaceEmbeddings(\n",
        "                    model_name=self.embedding_model_name,\n",
        "                    model_kwargs={\"device\": self.device}\n",
        "                )\n",
        "            except ImportError:\n",
        "                raise RuntimeError(\n",
        "                    \"sentence-transformers is not installed. \"\n",
        "                    \"Install it using: pip install sentence-transformers\"\n",
        "                )\n",
        "            print(\"Embedding model loaded\")\n",
        "\n",
        "    def _load_qa_model(self):\n",
        "        \"\"\"Load the CausalLM model for answer generation\"\"\"\n",
        "        if self.qa_model is None or self.tokenizer is None:\n",
        "            print(\"Loading Causal LM model...\")\n",
        "\n",
        "            # Load tokenizer\n",
        "            self.tokenizer = AutoTokenizer.from_pretrained(self.qa_model_name)\n",
        "\n",
        "            # Set pad token if not exists\n",
        "            if self.tokenizer.pad_token is None:\n",
        "                self.tokenizer.pad_token = self.tokenizer.eos_token\n",
        "\n",
        "            # Load model with optimizations for T4 GPU\n",
        "            self.qa_model = AutoModelForCausalLM.from_pretrained(\n",
        "                self.qa_model_name,\n",
        "                torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
        "                device_map=\"auto\" if torch.cuda.is_available() else None,\n",
        "                low_cpu_mem_usage=True\n",
        "            )\n",
        "\n",
        "            # Load PEFT adapter if specified\n",
        "            if self.qa_peft_adapter:\n",
        "                print(f\"Loading PEFT adapter: {self.qa_peft_adapter}\")\n",
        "                self.qa_model = PeftModel.from_pretrained(self.qa_model, self.qa_peft_adapter)\n",
        "\n",
        "            if not torch.cuda.is_available():\n",
        "                self.qa_model.to(self.device)\n",
        "\n",
        "            # Update generation config with tokenizer info\n",
        "            self.generation_config.pad_token_id = self.tokenizer.pad_token_id\n",
        "            self.generation_config.eos_token_id = self.tokenizer.eos_token_id\n",
        "\n",
        "            print(\"Causal LM model loaded\")\n",
        "\n",
        "    def _unload_qa_model(self):\n",
        "        \"\"\"Unload QA model to free memory\"\"\"\n",
        "        if self.qa_model is not None:\n",
        "            del self.qa_model\n",
        "            self.qa_model = None\n",
        "        if self.tokenizer is not None:\n",
        "            del self.tokenizer\n",
        "            self.tokenizer = None\n",
        "        gc.collect()\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "    def add_documents(self, documents):\n",
        "        \"\"\"Add documents to the knowledge base\"\"\"\n",
        "        try:\n",
        "            self._load_embedding_model()\n",
        "\n",
        "            # Flatten and chunk text\n",
        "            all_chunks = []\n",
        "            for doc in documents:\n",
        "                chunks = self.text_splitter.split_text(doc[\"content\"])\n",
        "                for chunk in chunks:\n",
        "                    all_chunks.append(\n",
        "                        LangchainDocument(page_content=chunk, metadata=doc[\"metadata\"])\n",
        "                    )\n",
        "\n",
        "            if not all_chunks:\n",
        "                return {\"status\": \"error\", \"message\": \"No valid document content to index.\"}\n",
        "\n",
        "            if self.vector_store is None:\n",
        "                self.vector_store = FAISS.from_documents(all_chunks, self.embedding_model)\n",
        "            else:\n",
        "                self.vector_store.add_documents(all_chunks)\n",
        "\n",
        "            return {\n",
        "                \"status\": \"success\",\n",
        "                \"message\": f\"Added {len(documents)} documents with {len(all_chunks)} total chunks.\"\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            return {\n",
        "                \"status\": \"error\",\n",
        "                \"message\": f\"Failed to add documents: {str(e)}\"\n",
        "            }\n",
        "\n",
        "    def _create_prompt(self, question, context):\n",
        "        \"\"\"Create a structured prompt for the language model\"\"\"\n",
        "        prompt = f\"\"\"Context: {context}\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Answer: Based on the provided context, \"\"\"\n",
        "        return prompt\n",
        "\n",
        "    def _clean_generated_answer(self, generated_text, original_prompt):\n",
        "        \"\"\"Clean and extract the answer from generated text\"\"\"\n",
        "        # Remove the original prompt from the generated text\n",
        "        if original_prompt in generated_text:\n",
        "            answer = generated_text.replace(original_prompt, \"\").strip()\n",
        "        else:\n",
        "            answer = generated_text.strip()\n",
        "\n",
        "        # Remove any remaining prompt artifacts\n",
        "        answer = re.sub(r'^Answer:\\s*', '', answer)\n",
        "        answer = re.sub(r'^Based on the provided context,?\\s*', '', answer, flags=re.IGNORECASE)\n",
        "\n",
        "        # Clean up common generation artifacts\n",
        "        answer = re.sub(r'\\n+', ' ', answer)  # Replace multiple newlines with space\n",
        "        answer = re.sub(r'\\s+', ' ', answer)  # Replace multiple spaces with single space\n",
        "\n",
        "        # Truncate at sentence boundaries if too long\n",
        "        sentences = answer.split('.')\n",
        "        if len(sentences) > 3:\n",
        "            answer = '. '.join(sentences[:3]) + '.'\n",
        "\n",
        "        return answer.strip()\n",
        "\n",
        "    def answer_question(self, question, top_k=3, max_context_length=800):\n",
        "        \"\"\"Generate an abstractive answer to a question using retrieved context\"\"\"\n",
        "        try:\n",
        "            if self.vector_store is None:\n",
        "                return {\"status\": \"error\", \"message\": \"Knowledge base is empty.\"}\n",
        "\n",
        "            self._load_embedding_model()\n",
        "\n",
        "            # Retrieve relevant documents\n",
        "            docs = self.vector_store.similarity_search(question, k=top_k)\n",
        "            contexts = [doc.page_content for doc in docs]\n",
        "            combined_context = \" \".join(contexts)\n",
        "\n",
        "            # Truncate context if too long\n",
        "            if len(combined_context) > max_context_length:\n",
        "                combined_context = combined_context[:max_context_length] + \"...\"\n",
        "\n",
        "            self._load_qa_model()\n",
        "\n",
        "            # Create prompt\n",
        "            prompt = self._create_prompt(question, combined_context)\n",
        "\n",
        "            # Tokenize input\n",
        "            inputs = self.tokenizer(\n",
        "                prompt,\n",
        "                return_tensors=\"pt\",\n",
        "                max_length=512,\n",
        "                truncation=True,\n",
        "                padding=True\n",
        "            )\n",
        "\n",
        "            # Move to device\n",
        "            input_ids = inputs[\"input_ids\"].to(self.device)\n",
        "            attention_mask = inputs[\"attention_mask\"].to(self.device)\n",
        "\n",
        "            # Generate answer\n",
        "            with torch.no_grad():\n",
        "                outputs = self.qa_model.generate(\n",
        "                    input_ids=input_ids,\n",
        "                    attention_mask=attention_mask,\n",
        "                    generation_config=self.generation_config,\n",
        "                    return_dict_in_generate=True,\n",
        "                    output_scores=True\n",
        "                )\n",
        "\n",
        "            # Decode generated text\n",
        "            generated_ids = outputs.sequences[0]\n",
        "            generated_text = self.tokenizer.decode(generated_ids, skip_special_tokens=True)\n",
        "\n",
        "            # Clean the answer\n",
        "            answer = self._clean_generated_answer(generated_text, prompt)\n",
        "\n",
        "            if not answer.strip():\n",
        "                answer = \"I don't have enough information to provide a comprehensive answer to that question.\"\n",
        "\n",
        "            result = {\n",
        "                \"status\": \"success\",\n",
        "                \"answer\": answer,\n",
        "                \"sources\": [{\"content\": doc.page_content, \"metadata\": doc.metadata} for doc in docs],\n",
        "                \"context_used\": combined_context[:200] + \"...\" if len(combined_context) > 200 else combined_context  # For debugging\n",
        "            }\n",
        "\n",
        "            return result\n",
        "\n",
        "        except Exception as e:\n",
        "            return {\"status\": \"error\", \"message\": f\"Error generating answer: {str(e)}\"}\n",
        "\n",
        "        finally:\n",
        "            self._unload_qa_model()\n",
        "\n",
        "    def get_vector_store_info(self):\n",
        "        \"\"\"Get information about the current vector store\"\"\"\n",
        "        if self.vector_store is None:\n",
        "            return {\"status\": \"empty\", \"message\": \"No documents in knowledge base\"}\n",
        "\n",
        "        try:\n",
        "            # Get the number of documents in the vector store\n",
        "            doc_count = self.vector_store.index.ntotal if hasattr(self.vector_store, 'index') else \"Unknown\"\n",
        "            return {\n",
        "                \"status\": \"ready\",\n",
        "                \"document_count\": doc_count,\n",
        "                \"embedding_model\": self.embedding_model_name\n",
        "            }\n",
        "        except Exception as e:\n",
        "            return {\"status\": \"error\", \"message\": f\"Error getting vector store info: {str(e)}\"}\n",
        "\n",
        "    def clear_knowledge_base(self):\n",
        "        \"\"\"Clear the vector store and free memory\"\"\"\n",
        "        if self.vector_store is not None:\n",
        "            del self.vector_store\n",
        "            self.vector_store = None\n",
        "\n",
        "        gc.collect()\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "        return {\"status\": \"success\", \"message\": \"Knowledge base cleared\"}"
      ],
      "metadata": {
        "id": "_NS9XlHMtQIK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Summarization Module with PEFT Support\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from peft import PeftModel\n",
        "import torch\n",
        "import gc\n",
        "\n",
        "class TextSummarizer:\n",
        "    def __init__(self):\n",
        "        self.model_name = MODEL_CONFIG[\"summarization_model\"]\n",
        "        self.peft_adapter = MODEL_CONFIG[\"summarization_peft_adapter\"]\n",
        "        self.tokenizer = None\n",
        "        self.model = None\n",
        "        print(f\"Summarizer initialized with model '{self.model_name}' - model will be loaded when needed\")\n",
        "\n",
        "    def _load_model(self):\n",
        "        if self.tokenizer is None or self.model is None:\n",
        "            print(\"Loading summarization model...\")\n",
        "            self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
        "            self.model = AutoModelForSeq2SeqLM.from_pretrained(self.model_name)\n",
        "\n",
        "            # Load PEFT adapter if specified\n",
        "            if self.peft_adapter:\n",
        "                print(f\"Loading PEFT adapter: {self.peft_adapter}\")\n",
        "                self.model = PeftModel.from_pretrained(self.model, self.peft_adapter)\n",
        "\n",
        "            if torch.cuda.is_available():\n",
        "                self.model = self.model.to(\"cuda\")\n",
        "            print(\"Model loaded\")\n",
        "\n",
        "    def _unload_model(self):\n",
        "        self.tokenizer = None\n",
        "        self.model = None\n",
        "        gc.collect()\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "    def _smart_chunk(self, text, chunk_size=512, chunk_overlap=50):\n",
        "        splitter = RecursiveCharacterTextSplitter(\n",
        "            chunk_size=chunk_size,\n",
        "            chunk_overlap=chunk_overlap,\n",
        "            separators=[\"\\n\\n\", \"\\n\", \".\", \"!\", \"?\", \",\", \" \"],\n",
        "        )\n",
        "        return splitter.split_text(text)\n",
        "\n",
        "    def summarize(self, text, max_length=150, min_length=40):\n",
        "        try:\n",
        "            self._load_model()\n",
        "\n",
        "            chunks = self._smart_chunk(text)\n",
        "            summaries = []\n",
        "\n",
        "            for chunk in chunks:\n",
        "                inputs = self.tokenizer(chunk, return_tensors=\"pt\", truncation=True, max_length=512)\n",
        "                if torch.cuda.is_available():\n",
        "                    inputs = {k: v.to(\"cuda\") for k, v in inputs.items()}\n",
        "\n",
        "                summary_ids = self.model.generate(\n",
        "                    inputs[\"input_ids\"],\n",
        "                    max_length=max_length,\n",
        "                    min_length=min_length,\n",
        "                    num_beams=4,\n",
        "                    length_penalty=2.0,\n",
        "                    early_stopping=True\n",
        "                )\n",
        "\n",
        "                summary = self.tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "                summaries.append(summary)\n",
        "\n",
        "            final_summary = \" \".join(summaries)\n",
        "\n",
        "            self._unload_model()\n",
        "\n",
        "            return {\n",
        "                \"status\": \"success\",\n",
        "                \"summary\": final_summary,\n",
        "                \"note\": f\"Processed in {len(chunks)} chunk(s) using LangChain chunking\"\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            self._unload_model()\n",
        "            return {\n",
        "                \"status\": \"error\",\n",
        "                \"message\": str(e)\n",
        "            }\n"
      ],
      "metadata": {
        "id": "aIZgaqcODBGV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sentiment Classification Module\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline\n",
        "from peft import PeftModel\n",
        "import torch\n",
        "import gc\n",
        "\n",
        "class SentimentClassifier:\n",
        "    def __init__(self):\n",
        "        self.model_name = MODEL_CONFIG[\"sentiment_model\"]\n",
        "        self.peft_adapter = MODEL_CONFIG[\"sentiment_peft_adapter\"]\n",
        "        self.tokenizer = None\n",
        "        self.model = None\n",
        "        self.pipeline = None\n",
        "        print(f\"Sentiment Classifier initialized with model '{self.model_name}' - model will be loaded when needed\")\n",
        "\n",
        "    def _load_model(self):\n",
        "        if self.tokenizer is None or self.model is None:\n",
        "            print(\"Loading sentiment classification model...\")\n",
        "            self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
        "            self.model = AutoModelForSequenceClassification.from_pretrained(self.model_name)\n",
        "\n",
        "            # Load PEFT adapter if specified\n",
        "            if self.peft_adapter:\n",
        "                print(f\"Loading PEFT adapter: {self.peft_adapter}\")\n",
        "                self.model = PeftModel.from_pretrained(self.model, self.peft_adapter)\n",
        "\n",
        "            device = 0 if torch.cuda.is_available() else -1\n",
        "            self.pipeline = pipeline(\n",
        "                \"sentiment-analysis\",\n",
        "                model=self.model,\n",
        "                tokenizer=self.tokenizer,\n",
        "                device=device\n",
        "            )\n",
        "            print(\"Sentiment model loaded\")\n",
        "\n",
        "    def _unload_model(self):\n",
        "        self.tokenizer = None\n",
        "        self.model = None\n",
        "        self.pipeline = None\n",
        "        gc.collect()\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "    def classify_sentiment(self, text):\n",
        "        try:\n",
        "            self._load_model()\n",
        "\n",
        "            # Handle text length\n",
        "            if len(text) > 512:\n",
        "                text = text[:512]\n",
        "\n",
        "            result = self.pipeline(text)\n",
        "\n",
        "            # Standardize output format\n",
        "            sentiment_result = {\n",
        "                \"status\": \"success\",\n",
        "                \"text\": text,\n",
        "                \"sentiment\": result[0][\"label\"].lower(),\n",
        "                \"confidence\": round(result[0][\"score\"], 4),\n",
        "                \"raw_output\": result\n",
        "            }\n",
        "\n",
        "            return sentiment_result\n",
        "\n",
        "        except Exception as e:\n",
        "            return {\n",
        "                \"status\": \"error\",\n",
        "                \"message\": str(e)\n",
        "            }\n",
        "        finally:\n",
        "            self._unload_model()\n",
        "\n",
        "    def batch_classify_sentiment(self, texts):\n",
        "        try:\n",
        "            self._load_model()\n",
        "\n",
        "            # Truncate texts if needed\n",
        "            processed_texts = [text[:512] if len(text) > 512 else text for text in texts]\n",
        "\n",
        "            results = self.pipeline(processed_texts)\n",
        "\n",
        "            batch_results = []\n",
        "            for i, result in enumerate(results):\n",
        "                batch_results.append({\n",
        "                    \"text\": processed_texts[i],\n",
        "                    \"sentiment\": result[\"label\"].lower(),\n",
        "                    \"confidence\": round(result[\"score\"], 4)\n",
        "                })\n",
        "\n",
        "            return {\n",
        "                \"status\": \"success\",\n",
        "                \"results\": batch_results,\n",
        "                \"count\": len(batch_results)\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            return {\n",
        "                \"status\": \"error\",\n",
        "                \"message\": str(e)\n",
        "            }\n",
        "        finally:\n",
        "            self._unload_model()"
      ],
      "metadata": {
        "id": "p06miQK2DfiI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Named Entity Recognition Module\n",
        "from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline\n",
        "from peft import PeftModel\n",
        "import torch\n",
        "import gc\n",
        "\n",
        "class NamedEntityRecognizer:\n",
        "    def __init__(self):\n",
        "        self.model_name = MODEL_CONFIG[\"ner_model\"]\n",
        "        self.peft_adapter = MODEL_CONFIG[\"ner_peft_adapter\"]\n",
        "        self.tokenizer = None\n",
        "        self.model = None\n",
        "        self.pipeline = None\n",
        "        print(f\"NER initialized with model '{self.model_name}' - model will be loaded when needed\")\n",
        "\n",
        "    def _load_model(self):\n",
        "        if self.tokenizer is None or self.model is None:\n",
        "            print(\"Loading NER model...\")\n",
        "            self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
        "            self.model = AutoModelForTokenClassification.from_pretrained(self.model_name)\n",
        "\n",
        "            # Load PEFT adapter if specified\n",
        "            if self.peft_adapter:\n",
        "                print(f\"Loading PEFT adapter: {self.peft_adapter}\")\n",
        "                self.model = PeftModel.from_pretrained(self.model, self.peft_adapter)\n",
        "\n",
        "            device = 0 if torch.cuda.is_available() else -1\n",
        "            self.pipeline = pipeline(\n",
        "                \"ner\",\n",
        "                model=self.model,\n",
        "                tokenizer=self.tokenizer,\n",
        "                device=device,\n",
        "                aggregation_strategy=\"simple\"\n",
        "            )\n",
        "            print(\"NER model loaded\")\n",
        "\n",
        "    def _unload_model(self):\n",
        "        self.tokenizer = None\n",
        "        self.model = None\n",
        "        self.pipeline = None\n",
        "        gc.collect()\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "    def extract_entities(self, text):\n",
        "        try:\n",
        "            self._load_model()\n",
        "\n",
        "            # Handle text length\n",
        "            if len(text) > 512:\n",
        "                text = text[:512]\n",
        "\n",
        "            entities = self.pipeline(text)\n",
        "\n",
        "            # Process and clean entities\n",
        "            processed_entities = []\n",
        "            for entity in entities:\n",
        "                processed_entities.append({\n",
        "                    \"text\": entity[\"word\"],\n",
        "                    \"label\": entity[\"entity_group\"],\n",
        "                    \"confidence\": round(float(entity[\"score\"]), 4),\n",
        "                    \"start\": entity[\"start\"],\n",
        "                    \"end\": entity[\"end\"]\n",
        "                })\n",
        "\n",
        "            # Group entities by type\n",
        "            entities_by_type = {}\n",
        "            for entity in processed_entities:\n",
        "                entity_type = entity[\"label\"]\n",
        "                if entity_type not in entities_by_type:\n",
        "                    entities_by_type[entity_type] = []\n",
        "                entities_by_type[entity_type].append(entity)\n",
        "\n",
        "            return {\n",
        "                \"status\": \"success\",\n",
        "                \"text\": text,\n",
        "                \"entities\": processed_entities,\n",
        "                \"entities_by_type\": entities_by_type,\n",
        "                \"entity_count\": len(processed_entities)\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            return {\n",
        "                \"status\": \"error\",\n",
        "                \"message\": str(e)\n",
        "            }\n",
        "        finally:\n",
        "            self._unload_model()\n",
        "\n",
        "    def batch_extract_entities(self, texts):\n",
        "        try:\n",
        "            self._load_model()\n",
        "\n",
        "            batch_results = []\n",
        "            for text in texts:\n",
        "                # Handle text length\n",
        "                if len(text) > 512:\n",
        "                    text = text[:512]\n",
        "\n",
        "                entities = self.pipeline(text)\n",
        "\n",
        "                processed_entities = []\n",
        "                for entity in entities:\n",
        "                    processed_entities.append({\n",
        "                        \"text\": entity[\"word\"],\n",
        "                        \"label\": entity[\"entity_group\"],\n",
        "                        \"confidence\": round(entity[\"score\"], 4),\n",
        "                        \"start\": entity[\"start\"],\n",
        "                        \"end\": entity[\"end\"]\n",
        "                    })\n",
        "\n",
        "                batch_results.append({\n",
        "                    \"text\": text,\n",
        "                    \"entities\": processed_entities,\n",
        "                    \"entity_count\": len(processed_entities)\n",
        "                })\n",
        "\n",
        "            return {\n",
        "                \"status\": \"success\",\n",
        "                \"results\": batch_results,\n",
        "                \"total_texts\": len(batch_results)\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            return {\n",
        "                \"status\": \"error\",\n",
        "                \"message\": str(e)\n",
        "            }\n",
        "        finally:\n",
        "            self._unload_model()\n"
      ],
      "metadata": {
        "id": "JXkDGDVaDkYO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# API Creation\n",
        "from fastapi import FastAPI, File, UploadFile, Form, HTTPException, Body\n",
        "from fastapi.middleware.cors import CORSMiddleware\n",
        "from pydantic import BaseModel\n",
        "from typing import List, Dict, Any, Optional\n",
        "import uvicorn\n",
        "import json\n",
        "import gc\n",
        "import torch\n",
        "\n",
        "app = FastAPI(title=\"NLP Performance Testing API with Abstractive RAG\")\n",
        "\n",
        "# Enable CORS\n",
        "app.add_middleware(\n",
        "    CORSMiddleware,\n",
        "    allow_origins=[\"*\"],\n",
        "    allow_credentials=True,\n",
        "    allow_methods=[\"*\"],\n",
        "    allow_headers=[\"*\"],\n",
        ")\n",
        "\n",
        "# Initialize our modules\n",
        "summarizer = None\n",
        "rag_system = None\n",
        "abstractive_rag_system = None\n",
        "sentiment_classifier = None\n",
        "ner_system = None\n",
        "\n",
        "@app.on_event(\"startup\")\n",
        "async def startup_event():\n",
        "    import subprocess\n",
        "    import sys\n",
        "\n",
        "    # Force install inside FastAPI process\n",
        "    subprocess.call([sys.executable, \"-m\", \"pip\", \"install\", \"sentence-transformers\"])\n",
        "    global summarizer, rag_system, abstractive_rag_system, sentiment_classifier, ner_system\n",
        "\n",
        "    summarizer = TextSummarizer()\n",
        "    rag_system = RAGSystem()\n",
        "    abstractive_rag_system = AbstractiveRAGSystem()\n",
        "    sentiment_classifier = SentimentClassifier()\n",
        "    ner_system = NamedEntityRecognizer()\n",
        "    print(\"API initialized - models will be loaded on demand\")\n",
        "\n",
        "# Define request models\n",
        "class Document(BaseModel):\n",
        "    content: str\n",
        "    metadata: Dict[str, Any]\n",
        "\n",
        "class QuestionRequest(BaseModel):\n",
        "    question: str\n",
        "    top_k: Optional[int] = 3\n",
        "    max_context_length: Optional[int] = 800\n",
        "\n",
        "class SummarizeRequest(BaseModel):\n",
        "    text: str\n",
        "    max_length: Optional[int] = 150\n",
        "    min_length: Optional[int] = 40\n",
        "\n",
        "class SentimentRequest(BaseModel):\n",
        "    text: str\n",
        "\n",
        "class BatchSentimentRequest(BaseModel):\n",
        "    texts: List[str]\n",
        "\n",
        "class NERRequest(BaseModel):\n",
        "    text: str\n",
        "\n",
        "class BatchNERRequest(BaseModel):\n",
        "    texts: List[str]\n",
        "\n",
        "# Define API endpoints\n",
        "\n",
        "# Extractive RAG endpoints\n",
        "@app.post(\"/rag/add_documents\")\n",
        "async def add_documents(documents: List[Document]):\n",
        "    \"\"\"Add documents to the extractive RAG knowledge base\"\"\"\n",
        "    gc.collect()\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "    plain_docs = [doc.dict() for doc in documents]\n",
        "    result = rag_system.add_documents(plain_docs)\n",
        "    return result\n",
        "\n",
        "@app.post(\"/rag/answer\")\n",
        "async def answer_question(request: QuestionRequest):\n",
        "    \"\"\"Answer questions using extractive RAG (span extraction)\"\"\"\n",
        "    gc.collect()\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "    result = rag_system.answer_question(request.question, top_k=request.top_k)\n",
        "    return result\n",
        "\n",
        "# Abstractive RAG endpoints\n",
        "@app.post(\"/abstractive_rag/add_documents\")\n",
        "async def add_documents_abstractive(documents: List[Document]):\n",
        "    \"\"\"Add documents to the abstractive RAG knowledge base\"\"\"\n",
        "    gc.collect()\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "    plain_docs = [doc.dict() for doc in documents]\n",
        "    result = abstractive_rag_system.add_documents(plain_docs)\n",
        "    return result\n",
        "\n",
        "@app.post(\"/abstractive_rag/answer\")\n",
        "async def answer_question_abstractive(request: QuestionRequest):\n",
        "    \"\"\"Answer questions using abstractive RAG (generative answers)\"\"\"\n",
        "    gc.collect()\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "    result = abstractive_rag_system.answer_question(\n",
        "        request.question,\n",
        "        top_k=request.top_k,\n",
        "        max_context_length=request.max_context_length\n",
        "    )\n",
        "    return result\n",
        "\n",
        "# Summarization endpoint\n",
        "@app.post(\"/summarize\")\n",
        "async def summarize_text(request: SummarizeRequest):\n",
        "    \"\"\"Summarize text using the summarization model\"\"\"\n",
        "    gc.collect()\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "    result = summarizer.summarize(\n",
        "        request.text,\n",
        "        max_length=request.max_length,\n",
        "        min_length=request.min_length\n",
        "    )\n",
        "    return result\n",
        "\n",
        "# Sentiment Analysis endpoints\n",
        "@app.post(\"/sentiment/classify\")\n",
        "async def classify_sentiment(request: SentimentRequest):\n",
        "    \"\"\"Classify sentiment of a single text\"\"\"\n",
        "    gc.collect()\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "    result = sentiment_classifier.classify_sentiment(request.text)\n",
        "    return result\n",
        "\n",
        "@app.post(\"/sentiment/batch_classify\")\n",
        "async def batch_classify_sentiment(request: BatchSentimentRequest):\n",
        "    \"\"\"Classify sentiment of multiple texts in batch\"\"\"\n",
        "    gc.collect()\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "    result = sentiment_classifier.batch_classify_sentiment(request.texts)\n",
        "    return result\n",
        "\n",
        "# Named Entity Recognition endpoints\n",
        "@app.post(\"/ner/extract\")\n",
        "async def extract_entities(request: NERRequest):\n",
        "    \"\"\"Extract named entities from a single text\"\"\"\n",
        "    gc.collect()\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "    result = ner_system.extract_entities(request.text)\n",
        "    return result\n",
        "\n",
        "@app.post(\"/ner/batch_extract\")\n",
        "async def batch_extract_entities(request: BatchNERRequest):\n",
        "    \"\"\"Extract named entities from multiple texts in batch\"\"\"\n",
        "    gc.collect()\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "    result = ner_system.batch_extract_entities(request.texts)\n",
        "    return result\n",
        "\n",
        "# Health check endpoint\n",
        "@app.get(\"/health\")\n",
        "async def health_check():\n",
        "    \"\"\"Check the health status of all available models\"\"\"\n",
        "    return {\n",
        "        \"status\": \"healthy\",\n",
        "        \"models_available\": {\n",
        "            \"summarizer\": summarizer is not None,\n",
        "            \"rag_system\": rag_system is not None,\n",
        "            \"abstractive_rag_system\": abstractive_rag_system is not None,\n",
        "            \"sentiment_classifier\": sentiment_classifier is not None,\n",
        "            \"ner_system\": ner_system is not None\n",
        "        },\n",
        "        \"endpoints\": {\n",
        "            \"extractive_rag\": [\"/rag/add_documents\", \"/rag/answer\"],\n",
        "            \"abstractive_rag\": [\"/abstractive_rag/add_documents\", \"/abstractive_rag/answer\"],\n",
        "            \"summarization\": [\"/summarize\"],\n",
        "            \"sentiment\": [\"/sentiment/classify\", \"/sentiment/batch_classify\"],\n",
        "            \"ner\": [\"/ner/extract\", \"/ner/batch_extract\"],\n",
        "            \"health\": [\"/health\"]\n",
        "        }\n",
        "    }\n",
        "\n",
        "# Additional utility endpoints\n",
        "@app.get(\"/models/info\")\n",
        "async def get_model_info():\n",
        "    \"\"\"Get information about the configured models\"\"\"\n",
        "    return {\n",
        "        \"model_config\": MODEL_CONFIG,\n",
        "        \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
        "        \"cuda_available\": torch.cuda.is_available(),\n",
        "        \"gpu_memory\": torch.cuda.get_device_properties(0).total_memory if torch.cuda.is_available() else None\n",
        "    }\n",
        "\n",
        "@app.post(\"/models/clear_cache\")\n",
        "async def clear_model_cache():\n",
        "    \"\"\"Clear GPU memory cache and run garbage collection\"\"\"\n",
        "    gc.collect()\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "        return {\n",
        "            \"status\": \"success\",\n",
        "            \"message\": \"GPU cache cleared and garbage collection run\",\n",
        "            \"gpu_memory_allocated\": torch.cuda.memory_allocated() if torch.cuda.is_available() else None\n",
        "        }\n",
        "    else:\n",
        "        return {\n",
        "            \"status\": \"success\",\n",
        "            \"message\": \"Garbage collection run (no GPU available)\",\n",
        "            \"gpu_memory_allocated\": None\n",
        "        }\n",
        "\n"
      ],
      "metadata": {
        "id": "a45vdZiIDrVo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 8: Deployment (Following Your Exact Structure)\n",
        "from pyngrok import ngrok\n",
        "import nest_asyncio\n",
        "import uvicorn\n",
        "\n",
        "# Create a public URL\n",
        "public_url = ngrok.connect(8000)\n",
        "print(f\"Public URL: {public_url}/docs\")\n",
        "\n",
        "# Apply nest_asyncio to allow nested event loops\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# Start the FastAPI server\n",
        "uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n"
      ],
      "metadata": {
        "id": "1fQo4bbpDxRX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VXpzDMYcGOMf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}